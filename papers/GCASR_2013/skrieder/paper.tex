\documentclass[conference]{IEEEtran}
% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi

\usepackage{pgf}
%\usepackage{tikz}
%\usetikzlibrary{arrows,automata}

\definecolor{darkgreen}{rgb}{0,0.7,0}

\newif\ifdraft
\drafttrue
%\draftfalse
\ifdraft
 \newcommand{\katznote}[1]{ {\textcolor{blue} { ***Dan:   #1 }}}
 \newcommand{\ketanote}[1]{{\textcolor{orange}  { ***Ketan:   #1 }}}
 \newcommand{\kriedernote}[1]{ {\textcolor{darkgreen}  { ***Scott:   #1 }}}
 \newcommand{\note}[1]{ {\textcolor{red}    {\bf #1 }}}
\else
 \newcommand{\katznote}[1]{}
 \newcommand{\kriedernote}[1]{}
 \newcommand{\note}[1]{}
\fi
% correct bad hyphenation here
%\hyphenation{op-tical net-works semi-conduc-tor}

\hyphenation{Queuing}

\begin{document}
%
% can use linebreaks \\ within to get better formatting as desired
\title{Towards Efficient Many-Task Computing on Accelerators in High-End Computing Systems}

%\author{\IEEEauthorblockN{Auth1\IEEEauthorrefmark{1},
%Auth2\IEEEauthorrefmark{1}\IEEEauthorrefmark{1}, 
%Auth3\IEEEauthorrefmark{1},
%\IEEEauthorblockA{\IEEEauthorrefmark{1}Argonne National Laboratory}
%}}

\author{Scott J. Krieder\IEEEauthorrefmark{1},
Benjamin Grimmer\IEEEauthorrefmark{1},
Dustin Shahidehpour\IEEEauthorrefmark{1},
Jeffrey Johnson\IEEEauthorrefmark{1}\\
Justin M. Wozniak\IEEEauthorrefmark{2},
Michael Wilde\IEEEauthorrefmark{2}\IEEEauthorrefmark{3},
Ioan Raicu\IEEEauthorrefmark{1}\IEEEauthorrefmark{2}\\
\IEEEauthorblockA{
\IEEEauthorrefmark{1}Department of Computer Science, Illinois Institute of Technology}
\IEEEauthorrefmark{2}MCS Division, Argonne National Laboratory\\
\IEEEauthorrefmark{3}Computation Institute, University of Chicago
}


\maketitle


\begin{abstract}
Current software and hardware limitations prevent Many-Task Computing (MTC) workloads from leveraging hardware accelerators boasting ManyCore Computing architectures. This work aims to address the programmability gap between MTC and accelerators, through the innovative CUDA middleware GeMTC. By working at the warp level, GeMTC enables heterogeneous task scheduling and 10x number of workers compared to CUDA. In order to span multiple accelerators across nodes, we have adopted the Swift parallel programming system, which can both support ﬁne grained millisecond tasks and extreme scale supercomputers at 100K-cores.
\end{abstract}

% no keywords
\begin{IEEEkeywords}
Many-Task Computing, Swift, GPGPU, CUDA
\end{IEEEkeywords}

\IEEEpeerreviewmaketitle

\section{Introduction}

\section{GeMTC}

\begin{figure}[h]
\centering\includegraphics[width=8cm]{imgs/big_picture.png}
\caption{Flow of a task through Swift/T and GeMTC.}
\label{fig:big_pic}
\end{figure}

\begin{figure}[h]
\centering\includegraphics[width=8cm]{imgs/warps.png}
\caption{Worker interaction with incoming work queue.}
\label{fig:warps}
\end{figure}

\begin{figure}[h]
\centering\includegraphics[width=8cm]{imgs/blockdiagram.png}
\caption{Kepler GK110 full chip block diagram.}
\label{fig:block_diagram}
\end{figure}

\section{Swift/T}

\begin{figure}[h]
\centering\includegraphics[width=8cm]{imgs/swiftt.png}
\caption{Overview of Swift/T toolchain.}
\label{fig:swiftt}
\end{figure}

Implicitly Parallel Scripting Language • Data-ﬂow driven scheduling of parallel tasks • Distributed executor eliminates centralized bottlenecks • Optimizing compiler detects errors, improves efﬁciency • Scales to 100k cores • Portable to most MPI-based clusters • Syntax similar to C, Java

\section{Preliminary Results}

\begin{figure}[h]
\centering\includegraphics[width=8cm]{imgs/1worker.png}
\caption{GeMTC efficiency from launching a single worker on the GPU.}
\label{fig:1worker}
\end{figure}

\begin{figure}[h]
\centering\includegraphics[width=8cm]{imgs/84workers.png}
\caption{GeMTC efficiency launching maximum workers on GTX680.}
\label{fig:84workers}
\end{figure}

\begin{figure}[h]
\centering\includegraphics[width=8cm]{imgs/multinode.png}
\caption{GeMTC + Swift/T efficiency on 4 XK7 nodes.}
\label{fig:multinode}
\end{figure}

\begin{figure}[h]
\centering\includegraphics[width=8cm]{imgs/throughput4nodes.png}
\caption{GeMTC + Swift/T throughput on 4 XK7 nodes.}
\label{fig:throughput}
\end{figure}

\section{Conclusions and Future Work}
Designed GeMTC Framework
• Improved Memory Management
• Integrated GeMTC + Swift/T
• Evaluated Synthetic Benchmarks

Abstract for other Accelerators
• Evaluate Real Applications
• Improve Current Performance

\section{Problem Statement}
This work aims to provide an integration between data-flow driven parallel programming systems (e.g. Many-Task Computing - MTC) and hardware accelerators \cite{kriederGCASR12} (e.g. NVIDIA GPUs, AMD GPUs, and the Intel MIC). MTC aims to bridge the gap between two computing paradigms, high throughput computing (HTC) and high-performance computing (HPC). MTC emphasizes using many computing resources over short periods of time to accomplish many computational tasks (i.e. including both dependent and independent tasks), where the primary metrics are measured in seconds.\cite{raicu2008toward} Swift is a particular implementation of the MTC paradigm, and is a parallel programming system that has been successfully used in many large-scale computing applications. \cite{zhao2007swift} The scientific community has adopted Swift as a great way to increase productivity in running complex applications via a dataflow driven programming model, which intrinsically allows implicit parallelism to be harnessed based on data access patterns and dependencies. Swift is a parallel programming system that fits the MTC model, and has been shown to run well on tens of thousands of nodes with task graphs in the range of hundreds of thousands of tasks. This work aims to enable Swift to efficiently use accelerators (such as NVIDIA GPUs and Intel MIC) to further accelerate a wide range of applications, on a growing portion of high-end systems.
\section{Description of Novel Approach}
The most recent version of Swift, namely Swift/T, supports function calls.\cite{wozniak13swift} By plugging our middleware into the Swift/T accelerator API(which we are currently pursuing a collaboration to develop) we plan to have Swift call C wrapper functions to CUDA kernels/applications directly. Currently CUDA developers may only have a maximum of 16 kernels running concurrently, one kernel per streaming multiprocessor (SM). The problem is that all kernels have to start and end at the same time, causing extreme inefficiencies in heterogeneous workloads. By working at the warp level we trade local memory for concurrency and we expect to be able to run up to 96 concurrent kernels. Our work will develop a middleware that will allow independent kernels (MIMD style) to be launched and managed on many-core architectures that traditionally only support SIMD. \cite{kriederXSEDE12}

We are also currently evaluating a real biochemistry application, namely the Open Protein Simulator (OOPS), which builds on the Protein Library (PL). OOPS is multipurpose and allows extensions to perform various simulation tasks relevant for life scientists, such as protein folding or protein structure prediction.\cite{OOPS} We have taken parts of this application and ported to NVIDIA GPUs via the CUDA programming language, in order to accelerate OOPS computations via Swift. We already have preliminary results in the costs associated with managing and launching concurrent kernels on NVIDIA FERMI GPUs, through the Swift system. We expect our results to be applicable to many HPC resources where GPUs are now common. For example the June 2012 Top 500 machines that are GPU enabled include Tianhe-1A, Jaguar, and Nebulae. Currently, Swift can only utilize the general processors on these machines to execute workloads and the GPUs are left idle. We will also explore many more applications from different domains, such as medicine, economics, astronomy, bioinformatics, physics, and many more.
\section{Contributions}
We plan to continue to push the performance envelope by enabling many MTC applications and systems to leverage the growing number of accelerated high-end computing systems. We also expect this work to enable other classes of applications to leverage accelerators, such as MapReduce and ensemble MPI. We also hope to influence future accelerator architectures by highlighting the need for hardware support for MIMD workloads.

\bibliographystyle{IEEEtran}
\bibliography{ref}
\end{document}
